(window.webpackJsonp=window.webpackJsonp||[]).push([[86],{156:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return s})),n.d(t,"metadata",(function(){return l})),n.d(t,"toc",(function(){return c})),n.d(t,"default",(function(){return u}));var a=n(3),i=n(7),r=(n(0),n(214)),o=["components"],s={title:"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2)",author:"Milan Bhan",author_title:"Senior Data Science Consultant, PhD student",author_url:"mailto:inno@ekimetrics.com",header_image_url:"./img/blog/couv.jpg",tags:["NLP","Transformers","BERT","interpretability","explainability","XAI","attention"],draft:!1,description:"Two illustrations of how attention coefficients can be a source of interpretability",keywords:["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"]},l={permalink:"/blog/2022/11/18/Interpretability_sentiment_analysis_I",editUrl:"https://github.com/ekimetrics/ekimetrics.github.io/edit/master/website/blog/blog/2022-11-18-Interpretability_sentiment_analysis_I.md",source:"@site/blog/2022-11-18-Interpretability_sentiment_analysis_I.md",description:"Two illustrations of how attention coefficients can be a source of interpretability",date:"2022-11-18T00:00:00.000Z",tags:[{label:"NLP",permalink:"/blog/tags/nlp"},{label:"Transformers",permalink:"/blog/tags/transformers"},{label:"BERT",permalink:"/blog/tags/bert"},{label:"interpretability",permalink:"/blog/tags/interpretability"},{label:"explainability",permalink:"/blog/tags/explainability"},{label:"XAI",permalink:"/blog/tags/xai"},{label:"attention",permalink:"/blog/tags/attention"}],title:"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2)",readingTime:5.945,truncated:!0,prevItem:{title:"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)",permalink:"/blog/2022/11/26/Interpretability_sentiment_analysis_II"},nextItem:{title:"Newsletter for September 2022",permalink:"/blog/2022/09/20/newsletter_Sept-2022"}},c=[{value:"Summary",id:"summary",children:[]},{value:"The BERT architecture",id:"the-bert-architecture",children:[]},{value:"Fine tuning of BERT for sentiment analysis",id:"fine-tuning-of-bert-for-sentiment-analysis",children:[]},{value:"Recovery of attention coefficients",id:"recovery-of-attention-coefficients",children:[]},{value:"Next step",id:"next-step",children:[]},{value:"References",id:"references",children:[]}],p={toc:c};function u(e){var t=e.components,s=Object(i.a)(e,o);return Object(r.b)("wrapper",Object(a.a)({},p,s,{components:t,mdxType:"MDXLayout"}),Object(r.b)("h2",{id:"summary"},"Summary"),Object(r.b)("div",{align:"justify"},Object(r.b)("p",null,"We propose to illustrate how far BERT-type models can be considered as interpretable by design. We show that the attention coefficients specific to BERT architecture constitute a particularly rich piece of information that can be used to perform interpretability. There are mainly two ways to do interpretability: attribution and generation of counterfactual examples. Here we propose to evaluate how attention coefficients can form the basis of an attribution method. We will show in a second article how they can also be used to set up counterfactuals. ")),Object(r.b)("p",null," ",Object(r.b)("img",{alt:"screenshot-app",src:n(327).default})),Object(r.b)("h2",{id:"the-bert-architecture"},"The BERT architecture"),Object(r.b)("div",{align:"justify"},Object(r.b)("p",null,"An artificial neural network is a computer system inspired by the functioning of the human brain and biological neurons to learn specific tasks. The neural networks represent a subset of machine learning algorithms. In order to perform a learning task, the neural network spreads information through an elementary network, called a perceptron. The way in which information is diffused can be formalized through linear algebra and the manipulation of various activation functions. A neural network can be defined as an association of elementary objects called formal neurons, like the perceptron. There are several types of layers that can be part of a neural network:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Fully connected layers, which receive a vector as input, and produce a new vector as output by applying a linear combination and possibly an activation function;"),Object(r.b)("li",{parentName:"ul"},"Convolution layers, which learn localized patterns in space;"),Object(r.b)("li",{parentName:"ul"},"Attention layers, which model the general relations between different objects."))),Object(r.b)("div",{align:"justify"},Object(r.b)("p",null,"Attention mechanisms are particularly effective for natural language processing tasks. This is mainly due to the fact that they allow to properly model a word through mathematical representations. In particular, attention layers make it possible to assign a contextual representation of the word on a case-by-case basis. This makes it a much more efficient tool than Word2vec since the latter only models an average context, but does not adapt to the given situation. Attention mechanisms are at the heart of Transformers-type models as shown in the diagram below. The BERT model corresponds to a stack of the left part of the generic architecture of a Transformer ","[1]",".")),Object(r.b)("p",null," ",Object(r.b)("img",{alt:"screenshot-app",src:n(328).default})),Object(r.b)("h2",{id:"fine-tuning-of-bert-for-sentiment-analysis"},"Fine tuning of BERT for sentiment analysis"),Object(r.b)("div",{align:"justify"},Object(r.b)("p",null,"To illustrate how attention coefficients can be a source of interpretability in natural language processing, we propose to fine tune a DistilBERT for sentiment analysis. A DistilBERT is a distilled version of BERT. It is smaller, faster, cheaper, lighter and recovers 97% of BERT\u2019s performance on GLUE ","[2]",". A perfect compromise, in fact. Most transformers are available pre-trained on the Hugging Face transformers library ","[3]",". The objective is to perform supervised classification on the IMDB database to assess the sentiment associated with a movie review. An illustration of the dataset is shown below:"),Object(r.b)("p",null," ",Object(r.b)("img",{alt:"screenshot-app",src:n(329).default})),Object(r.b)("p",null," To do so, we import all the libraries needed.  In particular, the tokenizer DistilBertTokenizer and the pre-trained hugging face model TFDistilBertForSequenceClassification are used."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre"},"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nsentence_encoder = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', output_attentions = True)\n")),Object(r.b)("p",null,'The parameter "output_attention" must be equal to "True". It will allow us to retrieve the attention coefficients of the model. We add a dense layer with a softmax activation to fine tune the model to do sentiment analysis. In order to train the model, we use the following hyperparameters:\ninitial_lr = 1e-5\nn_epochs = 15\nbatch_size = 64\nrandom_seed = 42'),Object(r.b)("p",null,"Finally, we make evolve the learning and stop the learning process if the val_loss does not decrease after a certain number of iterations. "),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre"},"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose = 1,min_delta=0.005,patience=3, min_lr=3e-7)\n\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1, mode='auto',baseline=None, restore_best_weights=True)\n")),Object(r.b)("p",null,"We can finally fine tune the DistilBert."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre"},"history = model.fit(X_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(X_test, y_test), \n                    verbose=1,callbacks=[early_stop, reduce_lr])\n")),Object(r.b)("p",null,"We obtain a val_accurcay of 85%, which is sufficient for our further analysis. Note that a BERT or a RoBERTa would certainly have had a better val_loss, as they are more heavy and complex."),Object(r.b)("h2",{id:"recovery-of-attention-coefficients"},"Recovery of attention coefficients"),Object(r.b)("p",null,"Recovery of attention coefficients\nWe are now able to analyze the attention coefficients related to movie reviews. In order to retrieve it, We need to predict the sentiment associated to a review..\nThen, we select the layer(s) of attention to analyze. We focus here on the last layer of attention."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre"},'inputs = tokenizer.batch_encode_plus(reviews,truncation=True, \n                                     add_special_tokens = True, \n                                     max_length = max_len, \n                                     pad_to_max_length = True)\n\ntokenized = np.array(inputs["input_ids"]).astype("int32")\nattention_mask = np.array(inputs["attention_mask"]).astype("int32")\nencoded_att = model.layers[2](tokenized,attention_mask =attention_mask)\n#last attention layer\nlast_attention=encoded_att.attentions[-1]\n')),Object(r.b)("p",null,"We finally recovered the 12 attention matrices from the last layer of the DistilBert.",Object(r.b)("br",{parentName:"p"}),"\n","Interpreting through attention attribution\nA first way to take advantage of the attention coefficients is to directly look at their amount in order to evaluate if the right words stand out. We choose to calculate the average attention on all attention layers and heads. A more in-depth work of selection of the most relevant layer would allow to refine the interpretability method. Here, we limit ourselves to the most basic case."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre"},"a,b = [], []\nfor head in range(0,12) :\n    for i, elt in enumerate(inputs['input_ids'][0]):\n        if np.array(elt) != 1:\n            att = last_attention.numpy()[0,head][0][i]\n            a.append(tokenizer.decode([elt]) + '_' + str(i))\n            b.append(att)\n            \nattention_all_head=pd.DataFrame({\"Token\":a,\"Attention coefficient\":b})\n")),Object(r.b)("p",null,'In order to have the average attention, we groupby the attention score on all the layers and heads.\nWe finally have the average attention coefficients associated with the words of the film review. As an example, the attention coefficients associated with the following positive review is calculated:\n\u201cProbably my all time favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring . it just never gets old despite my having seen it some 15 or more times in the last 25 years . paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight . the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch . and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling . if i had a dozen thumbs they\u2019d all be up for this movie".\nThe review being long, we represent the text in color. The more red the color, the higher the associated attention coefficient. The result is shown below:'),Object(r.b)("p",null," ",Object(r.b)("img",{alt:"screenshot-app",src:n(330).default})),Object(r.b)("p",null,'We see that the word groups "favorite movie", "it just never gets old", "performance brings tears", or "it is believable and startling" stand out. This explains well why the algorithm evaluated the review as positive and what was the semantic field at the root of this prediction.. '),Object(r.b)("h2",{id:"next-step"},"Next step"),Object(r.b)("p",null,"We will show in a future article how attention coefficients are useful for generating counterfactual examples to explain the model prediction."),Object(r.b)("h2",{id:"references"},"References"),Object(r.b)("p",null,"[1]"," VASWANI, Ashish, SHAZEER, Noam, PARMAR, Niki, et al. Attention is all you need. Advances in neural information processing systems, 2017, vol. 30."),Object(r.b)("p",null,"[2]"," SANH, Victor, DEBUT, Lysandre, CHAUMOND, Julien, et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019."),Object(r.b)("p",null,"[3]"," Hugging face library ",Object(r.b)("a",{parentName:"p",href:"https://huggingface.co/"},"https://huggingface.co/"))))}u.isMDXComponent=!0},214:function(e,t,n){"use strict";n.d(t,"a",(function(){return u})),n.d(t,"b",(function(){return d}));var a=n(0),i=n.n(a);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var c=i.a.createContext({}),p=function(e){var t=i.a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},u=function(e){var t=p(e.components);return i.a.createElement(c.Provider,{value:t},e.children)},b={inlineCode:"code",wrapper:function(e){var t=e.children;return i.a.createElement(i.a.Fragment,{},t)}},f=i.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,o=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=p(n),f=a,d=u["".concat(o,".").concat(f)]||u[f]||b[f]||r;return n?i.a.createElement(d,s(s({ref:t},c),{},{components:n})):i.a.createElement(d,s({ref:t},c))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=f;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var c=2;c<r;c++)o[c]=n[c];return i.a.createElement.apply(null,o)}return i.a.createElement.apply(null,n)}f.displayName="MDXCreateElement"},327:function(e,t,n){"use strict";n.r(t),t.default=n.p+"assets/images/Image_1-4f9dbd1cf01699409e62050b63433960.jpg"},328:function(e,t,n){"use strict";n.r(t),t.default=n.p+"assets/images/Image_2-3ae3a962b8d25b8df96ed38b648465b2.jpg"},329:function(e,t,n){"use strict";n.r(t),t.default=n.p+"assets/images/Image_3-5154784fda191b99bf74ac11a362d962.jpg"},330:function(e,t,n){"use strict";n.r(t),t.default=n.p+"assets/images/Image_4-fc95973e15821ef99dbc2fd6e4a5b6c8.jpg"}}]);