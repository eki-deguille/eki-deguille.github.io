<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.70">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Eki.Lab Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Eki.Lab Blog Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-124520099-9","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script><title data-react-helmet="true">Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2) | Eki.Lab</title><meta data-react-helmet="true" property="og:title" content="Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2) | Eki.Lab"><meta data-react-helmet="true" name="description" content="Two illustrations of how attention coefficients can be a source of interpretability."><meta data-react-helmet="true" property="og:description" content="Two illustrations of how attention coefficients can be a source of interpretability."><meta data-react-helmet="true" property="og:image" content="http://ekimetrics.github.io/img/10-cubecube03.jpg"><meta data-react-helmet="true" name="twitter:image" content="http://ekimetrics.github.io/img/10-cubecube03.jpg"><meta data-react-helmet="true" name="twitter:image:alt" content="Image for Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2) | Eki.Lab"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" name="keywords" content="Data Science,EkiLab,Ekimetrics,Eki.Lab,Eki,Machine Learning,Artificial Intelligence,Data Science for business,Operational Research,Optimization,Knapsack problem,Deep Reinforcement Learning"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/styles.1412233f.css">
<link rel="preload" href="/styles.007dbbc5.js" as="script">
<link rel="preload" href="/runtime~main.9e0186bf.js" as="script">
<link rel="preload" href="/main.0d301dfe.js" as="script">
<link rel="preload" href="/1.ac7b31e0.js" as="script">
<link rel="preload" href="/2.0cf1f8c8.js" as="script">
<link rel="preload" href="/149.0e354023.js" as="script">
<link rel="preload" href="/01a85c17.5eb5e366.js" as="script">
<link rel="preload" href="/1be78505.70f0b56f.js" as="script">
<link rel="preload" href="/56ff6114.3da1bae4.js" as="script">
<link rel="preload" href="/6875c492.4c852196.js" as="script">
<link rel="preload" href="/a6aa9e1f.e556fdbc.js" as="script">
<link rel="preload" href="/bc11c333.55e8966e.js" as="script">
<link rel="preload" href="/c4f5d8e4.446f4256.js" as="script">
<link rel="preload" href="/ccc49370.ba2dd310.js" as="script">
<link rel="preload" href="/152.d0023317.js" as="script">
<link rel="preload" href="/ece8ad2d.39263b09.js" as="script">
<link rel="preload" href="/8e5a020f.2ab21d6c.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_11B0">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><strong class="navbar__title">Eki.Lab</strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/docs/">About us</a><a class="navbar__item navbar__link" href="/resources/">Resources</a><a href="https://ekimetrics.com/fr/carrieres/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Careers</a></div><div class="navbar__items navbar__items--right"><a href="https://ekimetrics.com/fr/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Ekimetrics website</a><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Contact us!</a><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><strong class="navbar__title">Eki.Lab</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/blog">Blog</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/">About us</a></li><li class="menu__list-item"><a class="menu__link" href="/resources/">Resources</a></li><li class="menu__list-item"><a href="https://ekimetrics.com/fr/carrieres/" target="_blank" rel="noopener noreferrer" class="menu__link">Careers</a></li><li class="menu__list-item"><a href="https://ekimetrics.com/fr/" target="_blank" rel="noopener noreferrer" class="menu__link">Ekimetrics website</a></li><li class="menu__list-item"><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="menu__link">Contact us!</a></li></ul></div></div></div></nav><div class="main-wrapper blog-wrapper"><div class="container container-wide margin-vert--lg"><div class="row"><div class="col col--2"></div><main class="col col--8"><article><header><h1 class="margin-bottom--sm blogPostTitle_kDB-">Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)</h1><div class="margin-vert--md"><p>Two illustrations of how attention coefficients can be a source of interpretability.</p><time datetime="2022-10-26T00:00:00.000Z" class="blogPostDate_2HVl">October 26, 2022  · 7 min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"><h4 class="avatar__name">Written by <a href="mailto:inno@ekimetrics.com" target="_blank" rel="noreferrer noopener">Milan Bhan</a></h4><small class="avatar__subtitle">Senior Data Science Consultant, PhD student</small></div></div><div class="margin-vert--md"><img class="img-blog-header" src="/./img/blog/interpretability_articles_2.jpg"></div></header><section class="markdown blog-article-custom"><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="summary"></a>Summary<a class="hash-link" href="#summary" title="Direct link to heading">#</a></h2><div align="justify"><p>We propose to illustrate how far BERT-type models can be considered as interpretable by design. We show that the attention coefficients specific to BERT architecture constitute a particularly rich piece of information that can be used to perform interpretability. There are mainly two ways to do interpretability: attribution and generation of counterfactual examples. In a first article, we showed how attention coefficients could be the basis of an attribution interpretability method. Here we propose to evaluate how they can also be used to set up counterfactuals. </p></div><div align="justify"><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="work-presented-in-the-previous-article"></a>Work presented in the previous article<a class="hash-link" href="#work-presented-in-the-previous-article" title="Direct link to heading">#</a></h2><p>Previously, the BERT [1] and DistilBERT [2] models have been mobilized to tackle the well-known problem of sentiment analysis. In particular, we have shown that the BERT and DistilBERT models contain within their architecture attention coefficients that can be at the heart of an attribution interpretability method. Starting from an initial text, a visualization of the weight assignment method was proposed. The more red the color, the higher the associated attention coefficient. </p><p> <img alt="screenshot-app" src="/assets/images/Image_2-fc95973e15821ef99dbc2fd6e4a5b6c8.jpg"></p><div align="center"> Figure 1 - Attention-Based token importance</div><p> </p><p> We saw that the word groups &quot;<em>favorite movie</em>&quot;, &quot;<em>it just never gets old</em>&quot;, &quot;<em>performance brings tears</em>&quot;, or &quot;<em>it is believable and startling</em>&quot; stood out. This explained well why the algorithm evaluated the review as positive and what was the semantic field at the root of this prediction. This work was done using the Hugging Face transformers library [3].</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="interpreting-through-counterfactual-generation"></a>Interpreting through counterfactual generation<a class="hash-link" href="#interpreting-through-counterfactual-generation" title="Direct link to heading">#</a></h2><p>Another way to do interpretability is to generate counterfactual examples. According to Judea Pearl, counterfactual &quot;involves answering questions which ask what might have been, had circumstances been different” [4]. Thus, the idea is to understand a prediction by generating a counterfactual example, resulting in an opposite prediction. In the context of natural language processing, it is therefore a matter of changing the right words in the review. In order to generate a counterfactual example, we propose the following methodology:</p><ul><li><p>Compute the attention coefficients of the tokens in a text corpus on each attention layer (6). The text corpus size must be statistically significant </p></li><li><p>Perform token clustering based on their 6-dimensional representation</p></li><li><p>Detect clusters associated with positively and negatively charged sentiment words</p></li><li><p>Replace the tokens with the highest average attention with their &quot;opposite token&quot; in their &quot;opposite cluster&quot;
This approach allows us to validate the interpretative strength of the tokens put forward by the attention coefficients, while illustrating what a close review would have been with an opposite sentiment.
We apply the methodology on a corpus of 1000 reviews. The clustering method used is the hierarchical ascending classification (HAC) and gives 3 clusters. The obtained clusters and the counterfactual generation procedure can be represented in 2 dimensions as follows:</p><p> <img alt="screenshot-app" src="/assets/images/Image_3-094ab2c3d6b69a58223f9a733aef845b.jpg"></p><div align="center"> Figure 2 - Token clusters &amp; replacements</div></li></ul><p>We then generate the counterfactual example of the review tested earlier by changing 2 words: </p><div align="center">delight ➡ torment<p>favorite ➡ worst</p></div><p>This gives us the following counterfactual example:</p><p>“<em>Probably my all time worst movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring . it just never gets old despite my having seen it some 15 or more times in the last 25 years . paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a torment. the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch . and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling . if i had a dozen thumbs they’d all be up for this movie</em>&quot;.</p><p>As the text is quite long, 2 tokens are not enough to change the feeling associated with the review. The probability score nevertheless drops significantly by 0.3pts.
One way to assess the quality of the generated counterfactual examples is to evaluate the proportion of reviews in a corpus whose associated sentiment has changed. The result can be represented as a &quot;counterfactual confusion matrix&quot; as follows:</p><p>One way to assess the quality of the generated counterfactual examples is to evaluate the proportion of reviews in a corpus whose associated sentiment has changed. The result can be represented as a &quot;counterfactual confusion matrix&quot; as follows:</p><p> <img alt="screenshot-app" src="/assets/images/Image_4-91c11a737d5e41b98a8657cd3bb17de7.jpg"></p><div align="center"> Table 1 - Counterfactual confusion matrix example</div><p>Where :</p><ul><li>X<sub>11</sub> represents the share of reviews whose initial associated sentiment and the sentiment of the counterfactual example are positive; sentiment has remained the same </li><li>X<sub>12</sub> represents the share of reviews whose sentiment changed from positive to negative; sentiment did change </li><li>X<sub>21</sub> represents the share of reviews whose sentiment changed from negative to positive; sentiment changed well</li><li>X<sub>22</sub> represents the share of reviews whose initial associated sentiment and the sentiment of the counterfactual example are negative; sentiment has remained the same</li></ul><p>We compute the &quot;counterfactual confusion matrix&quot; on the same text corpus that enabled us to perform clustering, picking 5 tokens for each review. The result is given below:</p><p> <img alt="screenshot-app" src="/assets/images/Image_5-de802c493bd17b6c40e5c5714774720e.jpg"></p><div align="center"> Table 2 - Actual counterfactual confusion matrix</div><p> </p>Thus, we see that changing the 5 tokens with the highest average attention produces a change in sentiment perception in 44% of cases. In particular, the rate of sentiment change for reviews initially perceived as positive is 31% while the rate of sentiment change for reviews initially perceived as negative is 53%. The change from negative to positive seems to be better achieved with our method.<p>We have shown that attention coefficients can be a source of interpretability. Used in the right way, the attention coefficients allow the detection of tokens with high predictive value. They can also be used to generate counterfactual examples in order to better understand what the sentence should have been in order to be associated with an opposite sentiment. The interest of the attention coefficients is reinforced by the &quot;counterfactual confusion matrix&quot;: The high transformation rate of the reviews&#x27; sentiments shows that the tokens selected thanks to the attention are strongly meaningful.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="next-step"></a>Next step<a class="hash-link" href="#next-step" title="Direct link to heading">#</a></h2><p>We plan to test other ways to generate counterfactual examples. One way would be to take advantage of the way DistilBert has been trained: the mask language modeling (MLM). The idea would be to mask the tokens with high average attention, and replace them with the tokens with the highest softmax in the &quot;opposite cluster&quot;. This would ensure the grammatical correctness of the generated counterfactual example. Finally, the generation of counterfactual examples can have other applications than interpretability. In particular, it becomes possible to perform data augmentation in order to give more examples to a model. It can mitigate biases by balancing the sentiments of biased discriminated populations. This would improve fairness indicators while not degrading accuracy. </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="references"></a>References<a class="hash-link" href="#references" title="Direct link to heading">#</a></h2><p>[1] VASWANI, Ashish, SHAZEER, Noam, PARMAR, Niki, et al. Attention is all you need. Advances in neural information processing systems, 2017, vol. 30.</p><p>[2] SANH, Victor, DEBUT, Lysandre, CHAUMOND, Julien, et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p><p>[3] Hugging face library <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">https://huggingface.co/</a></p><p>[4] PEARL, Judea et MACKENZIE, Dana. The book of why: the new science of cause and effect. Basic books, 2018</p></div></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/blog/tags/nlp">NLP</a><a class="margin-horiz--sm" href="/blog/tags/transformers">Transformers</a><a class="margin-horiz--sm" href="/blog/tags/bert">BERT</a><a class="margin-horiz--sm" href="/blog/tags/interpretability">interpretability</a><a class="margin-horiz--sm" href="/blog/tags/explainability">explainability</a><a class="margin-horiz--sm" href="/blog/tags/xai">XAI</a><a class="margin-horiz--sm" href="/blog/tags/attention">attention</a></div></footer></article><div class="margin-vert--xl"><nav class="pagination-nav" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/2022/11/10/creative_execution_and_marketing_effectiveness_part_I"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« Exploring the links between creative execution and marketing effectiveness - Part I: Detectron2 Pre-Trained Object Detection Models</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/2022/10/18/Interpretability_sentiment_analysis_I"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2) »</div></a></div></nav></div></main><div class="col col--2"><div class="tableOfContents_2xL- thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link">Summary</a></li><li><a href="#work-presented-in-the-previous-article" class="table-of-contents__link">Work presented in the previous article</a></li><li><a href="#interpreting-through-counterfactual-generation" class="table-of-contents__link">Interpreting through counterfactual generation</a></li><li><a href="#next-step" class="table-of-contents__link">Next step</a></li><li><a href="#references" class="table-of-contents__link">References</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">About us</h4><ul class="footer__items"><li class="footer__item"><a href="https://ekimetrics.com/who-we-are/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Who we are ?</a></li><li class="footer__item"><a href="https://ekimetrics.com/our-team/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Our team</a></li><li class="footer__item"><a href="https://ekimetrics.us13.list-manage.com/subscribe?u=85b8ce42caa0a733e98233bc4&amp;id=6355d0a6f9" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe to our newsletter</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Find us</h4><ul class="footer__items"><li class="footer__item"><a href="https://github.com/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github</a></li><li class="footer__item"><a href="https://ekimetrics.com/careers/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Careers</a></li><li class="footer__item"><a href="https://www.welcometothejungle.com/fr/companies/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Eki on Welcome to the jungle</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Contact</h4><ul class="footer__items"><li class="footer__item"><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Get in touch with our teams</a></li></ul></div></div><div class="footer__bottom text--center"></div></div></footer></div>
<script src="/styles.007dbbc5.js"></script>
<script src="/runtime~main.9e0186bf.js"></script>
<script src="/main.0d301dfe.js"></script>
<script src="/1.ac7b31e0.js"></script>
<script src="/2.0cf1f8c8.js"></script>
<script src="/149.0e354023.js"></script>
<script src="/01a85c17.5eb5e366.js"></script>
<script src="/1be78505.70f0b56f.js"></script>
<script src="/56ff6114.3da1bae4.js"></script>
<script src="/6875c492.4c852196.js"></script>
<script src="/a6aa9e1f.e556fdbc.js"></script>
<script src="/bc11c333.55e8966e.js"></script>
<script src="/c4f5d8e4.446f4256.js"></script>
<script src="/ccc49370.ba2dd310.js"></script>
<script src="/152.d0023317.js"></script>
<script src="/ece8ad2d.39263b09.js"></script>
<script src="/8e5a020f.2ab21d6c.js"></script>
</body>
</html>